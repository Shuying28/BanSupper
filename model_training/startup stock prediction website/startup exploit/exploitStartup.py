import time
import matplotlib
from selenium import webdriver
from bs4 import BeautifulSoup
import pandas as pd
import requests
from urllib.request import Request, urlopen
import json
import re
import glob
import os
import numpy as np
import matplotlib as mpl
from matplotlib import pyplot as plt
import warnings
from decimal import Decimal
import math
import seaborn as sns
from scipy.stats import chi2_contingency
from scipy import misc
from matplotlib.colors import ListedColormap
import sklearn
from selenium.webdriver.common.by import By
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.ensemble import RandomForestClassifier
from sklearn import linear_model, metrics, preprocessing
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import r2_score, f1_score
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import minmax_scale
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import GaussianNB
from IPython.display import Image, display
from collections import Counter
import pydotplus

warnings.filterwarnings("ignore")
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 500)

startup_companies_page = 'https://www.crunchbase.com/discover/organization.companies'

def get_driver():
    driver = webdriver.Chrome(executable_path=r'C:\Users\user\Downloads\chromedriver_win32\chromedriver.exe')
    driver.get(startup_companies_page)
    return driver

def infinite_scroll(driver):
    time.sleep(2)
    scroll_pause_time = 1
    screen_height = driver.execute_script("return window.screen.height;") # get the screen height of the website
    i = 2
    while(i < 50):
        # scroll one screen height each time
        driver.execute_script("window.scrollTo(0, {screen_height}*{i});".format(screen_height = screen_height, i = i))
        i += 1
        time.sleep(scroll_pause_time)
        scroll_height = driver.execute_script("return document.body.scrollHeight;")

driver = get_driver()

infinite_scroll(driver)

# LinkedIn login
driver = get_driver()
username = driver.find_element(By.ID, 'username')
username.send_keys('s2132542@siswa.um.edu.my')
# driver.find_element(By.ID, 'session_key')
time.sleep(0.5)

password = driver.find_element_by_id('password')
password.send_keys('Coffeecode0610~')

time.sleep(0.5)

logInButton = driver.find_element(By.CLASS_NAME, 'btn__primary--large')
logInButton.click()

time.sleep(3)

# Scraping - get companies urls
def load_soup_object(html_file_name):
    html = open(html_file_name, encoding="utf8")
    soup = BeautifulSoup(html, 'html.parser')
    return soup

# split requests to multiple files because the website will block us after some time of infinite scrolling

def extract_companies_urls(html_link):
    html_link = "./Data/companies_urls/" + html_link
    soup = load_soup_object(html_link)
    companies_div = soup("div", attrs = {"class": 'js-company-card-list'})[0]
    links = list()
    for row in companies_div('div', attrs = {"class": 'box-view-item'}):
        url = row('a')[0]['href']
        links.append(url)
    return links

# Each HTML page with multiple companies urls, we extracted the urls in to one
# big dataframe
list_50_to_90 = extract_companies_urls('Start-Up1950-1990.html')
list_91_to_00 = extract_companies_urls('Start-Up1991-2000.html')
list_01_to_05 = extract_companies_urls('Start-Up2001-2005.html')
list_05_to_09 = extract_companies_urls('Start-Up2005-2009.html')
list_10_to_11 = extract_companies_urls('Start-Up2010-2011.html')
list_12 = extract_companies_urls('Start-Up2012.html')
list_13 = extract_companies_urls('Start-Up2013.html')
list_14 =extract_companies_urls('Start-Up2014.html')
list_15 = extract_companies_urls('Start-Up2015.html')
list_16 = extract_companies_urls('Start-Up2016.html')
list_17 = extract_companies_urls('Start-Up2017.html')
list_18 = extract_companies_urls('Start-Up2018.html')
list_19 = extract_companies_urls('Start-Up2018.html')
list_20_22 = extract_companies_urls('Start-Up2020-2022.html')

link_list = list_50_to_90 + list_91_to_00 +list_01_to_05+list_05_to_09+ list_10_to_11 + list_12+list_13 + list_14 +list_15 +list_16 +list_17 +list_18 + list_19 +list_20_22

df = pd.DataFrame({'links': link_list})

df = df.drop_duplicates().copy()

df = df.reset_index(drop=True)

df.to_csv("companies_url.csv", index=False)

# Scraping - Company Page Scraping
def load_csv(fileName):
    return pd.read_csv(fileName)

def load_soup_html(url):
    html = requests.get(url)
    soup = BeautifulSoup(html.text, 'html.parser')
    return soup

df = load_csv("companies_urls.csv")


def get_df_companies(df):
    res_df = pd.DataFrame(columns=['COMPANIE_NAME', 'FOUNDED', 'BUSINESS_MODEL', 'EMPLOYEES', 'FUNDING_STAGE', 'RAISED'
        , 'PRODUCT_STAGE', 'NEWS_COUNT', 'FOUNDER_COUNT', 'TOTAL_ROUNDS', 'INVESTORS_COUNT'
        , 'COMPANY_STATUS', 'IS_ACADEMIC', 'SECTOR', 'TARGET_INDUSTORY'])
    for url in df['links']:
        print(url)
        soup = load_soup_html(url)

        company_dict = {
            "NAME": None,
            "FOUNDED": None,
            "BUSINESS MODEL": None,
            "EMPLOYEES": None,
            "FUNDING STAGE": None,
            "RAISED": 0,
            "PRODUCT STAGE": None,
            "NEWS": 0,
            "FOUNDERS": 0,
            "Total rounds": 0,
            "Investors": 0,
            "STATUS": "Active",
            "IS ACADEMIC": 0,
            "SECTOR": None,
            "TARGET INDUSTORY": None
        }

        name_div = soup("div", attrs={"class": 'top-profile-section'})
        if (not name_div):
            continue

        company_dict["NAME"] = soup("div", attrs={"class": 'top-profile-section'})[0]("div", attrs={"class": 'title'})[
            0].get_text().strip()

        COMPANY_PROFILE = soup("div", attrs={"class": 'metadata-wrapper'})
        if (len(COMPANY_PROFILE)):
            for div in COMPANY_PROFILE[0]("div", attrs={"class": 'item-bottom'}):
                company_dict.update({div.get_text().strip(): div.find_previous('div').get_text().strip()})

        if (soup("div", attrs={"id": "news-section"})):
            news_div = soup("div", attrs={"id": "news-section"})[0]
            news_str = news_div("div", attrs={"class": "section-text"})[0].get_text().strip()
            company_dict["NEWS"] = int(re.findall('\d+', news_str)[0])

        if (soup("div", attrs={"id": "team-section"})):
            team_div = soup("div", attrs={"id": "team-section"})[0]
            team_str = team_div("div", attrs={"class": "section-text"})[0].get_text().strip()
            company_dict["FOUNDERS"] = int(re.findall('\d+', team_str)[0])

        FUNDING_DATA = soup("div", attrs={"class": 'funding-metadata'})
        if (len(FUNDING_DATA)):
            for div in FUNDING_DATA[0]("div", attrs={"class": 'subtitle'}):
                funding_text = div.get_text().strip()
                if (funding_text == 'Total rounds' or funding_text == 'Investors'):
                    company_dict[funding_text] = div.find_previous('div').get_text().strip()

        if (soup.find_all("div", string=["Public"])):
            company_dict["STATUS"] = "Public"
        elif (soup.find_all("div", string=["Acquired by"])):
            company_dict["STATUS"] = "Acquired"
        elif (soup.find_all("div", string=["Not Active"])):
            company_dict["STATUS"] = "Not Active"

        if (soup.find_all("div", string=["Academic technology"])):
            company_dict["IS ACADEMIC"] = 1

        sector_div = soup.find_all("use", attrs={"xlink:href": "#icon-classification-sector"})
        if (sector_div):
            sector_div = sector_div[0].parent.parent.parent.parent("a", attrs={"class": "lead-title"})
            company_dict["SECTOR"] = sector_div[0].get_text().strip()

        industry_div = soup.find_all("use", attrs={"xlink:href": "#icon-classification-industry"})
        if (industry_div):
            industry_div = industry_div[0].parent.parent.parent.parent("a", attrs={"class": "lead-title"})
            company_dict["TARGET INDUSTORY"] = industry_div[0].get_text().strip()

        print(company_dict)
        res_df.loc[len(res_df)] = list(company_dict.values())

    return res_df

for i in range(0,104):
    df_res = get_df_companies(df.iloc[i*100: (i+1) * 100])
    df_res.to_csv("Data\companies_df\companies_" + str(i*100) + "-" + str((i*100) + 99) + ".csv",index = False)
    print("************************ file number:" + str(i) + "***********************\n\n")
    time.sleep(10)

path = r'Data\companies_df'
all_files = sorted(glob.glob(os.path.join(path, "*.csv")), key=os.path.getmtime) #get all files sorted by time modified asc

df_from_each_file = (pd.read_csv(f) for f in all_files)
concatenated_df = pd.concat(df_from_each_file, ignore_index=True)
# concatenated_df

filepath = r'Data\companies_df\main_df.csv'
concatenated_df.to_csv(filepath, index=False)

main_df = pd.read_csv('Data/companies_df/main_df.csv')

main_df